{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U keras-nlp\n!pip install -U keras\nimport keras\nimport keras_nlp\n\nimport numpy as np \nimport pandas as pd \nimport torch\npd.set_option('display.max_colwidth', 300)\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:10:56.803953Z","iopub.execute_input":"2024-06-06T07:10:56.804655Z","iopub.status.idle":"2024-06-06T07:11:58.788705Z","shell.execute_reply.started":"2024-06-06T07:10:56.804620Z","shell.execute_reply":"2024-06-06T07:11:58.787870Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras-nlp in /opt/conda/lib/python3.10/site-packages (0.9.3)\nCollecting keras-nlp\n  Downloading keras_nlp-0.12.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting keras-core (from keras-nlp)\n  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (2023.12.25)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (13.7.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\nRequirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (0.2.3)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (2.15.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.66.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.10.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-nlp) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp) (2.17.2)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-nlp) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-nlp) (2.15.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.51.1)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text->keras-nlp) (2.15.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2024.2.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.2.2)\nDownloading keras_nlp-0.12.1-py3-none-any.whl (570 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras, keras-core, keras-nlp\n  Attempting uninstall: keras\n    Found existing installation: keras 3.2.1\n    Uninstalling keras-3.2.1:\n      Successfully uninstalled keras-3.2.1\n  Attempting uninstall: keras-nlp\n    Found existing installation: keras-nlp 0.9.3\n    Uninstalling keras-nlp-0.9.3:\n      Successfully uninstalled keras-nlp-0.9.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 keras-core-0.1.7 keras-nlp-0.12.1\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.11.0)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.2.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.9.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-3.3.3\n","output_type":"stream"},{"name":"stderr","text":"2024-06-06 07:11:36.685983: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-06 07:11:36.686093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-06 07:11:36.969859: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U keras","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:14:14.603318Z","iopub.execute_input":"2024-06-06T07:14:14.604485Z","iopub.status.idle":"2024-06-06T07:14:26.688921Z","shell.execute_reply.started":"2024-06-06T07:14:14.604445Z","shell.execute_reply":"2024-06-06T07:14:26.687701Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.3.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.11.0)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.2.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.9.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Call PreTrained Model and Generate Summary**","metadata":{}},{"cell_type":"markdown","source":"Now I am going to call the model and generate the model summary. I chose the 2B model to get started, which is introducted in the Google Gemma Documentation [here](https://ai.google.dev/gemma/docs/?utm_source=agd&utm_medium=referral&utm_campaign=quickstart-docu&utm_content). Here is a quick summary:","metadata":{}},{"cell_type":"markdown","source":"* There are 2B paramters\n* Input: Text\n* Output: Text\n* Intended for Mobile devices and laptops\n* Available in Pretrained and instruction tuned","metadata":{}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n#AutoModelForCausalLM.from_pretrained(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:18:55.858338Z","iopub.execute_input":"2024-06-06T07:18:55.859441Z","iopub.status.idle":"2024-06-06T07:20:07.977655Z","shell.execute_reply.started":"2024-06-06T07:18:55.859375Z","shell.execute_reply":"2024-06-06T07:20:07.976847Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Summary**","metadata":{}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:44:32.139639Z","iopub.execute_input":"2024-05-25T19:44:32.140521Z","iopub.status.idle":"2024-05-25T19:44:32.176917Z","shell.execute_reply.started":"2024-05-25T19:44:32.140488Z","shell.execute_reply":"2024-05-25T19:44:32.176056Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### [About the Instruction Tuned Model](https://ai.google.dev/gemma/docs/formatting)\nThe instruction tuned (it) model is different from the base model specifically as it relates to formatting. According to this [Medium article](https://medium.com/@zaiinn440/everything-you-need-to-know-about-googles-new-gemma-7b-and-2b-models-5714dfaa48d7), the instruction tuned model is better for chat purposes because they were fine tuned to take in more directions and therefore generate better answers. The task that I'm going to attempt to complete with Gemma is \"Explain or teach basic data science concepts\". I am approaching this as a chat-based activity and therefore I decided to use the instruction tuned model.","metadata":{}},{"cell_type":"markdown","source":"### **Prompting the LLM**","metadata":{}},{"cell_type":"markdown","source":"Let's start by testing out a few prompts on the pretrained model. I have a lot of experience with writing and tutoring, so I had some fun coming up with different prompts based on my experience. Let's see what we can generate:","metadata":{}},{"cell_type":"markdown","source":"We're going to use the TopKSampler and TopPSampler. Documentation is available here for [TopK](https://keras.io/api/keras_nlp/samplers/top_k_sampler/#topksampler-class) and here for [TopP](https://keras.io/api/keras_nlp/samplers/top_p_sampler/). We can adjust the value for p , k as well as the temperature. I didn't know much about how this worked beforehand, so I did some research about the top K values and the temperature values for LLMs. I'm going to summarize what I found from the [OpenAI community](https://community.openai.com/t/temperature-top-p-and-top-k-for-chatbot-responses/295542).","metadata":{}},{"cell_type":"markdown","source":"> **Top P**: Nucleus sampling selects the most likely tokens from a probability distribution.\\\n> **Top K**: For our sampler, we use the k most likely next tokens at each step. Lower k focuses on \\higher probability tokens.\\\n> **Temperature**: Adjusting this value changes the randomness of responses. By increasing the value, your answers are generally going to be more diverse.","metadata":{}},{"cell_type":"markdown","source":"I found some more information on LinkedIn that talks about striking a balance between creating diverse, creative responses (high temperature) and restricting answers to the most likely optoiins (top K/p).","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install langchain-community langchain-core\nimport langchain\nfrom langchain import PromptTemplate, OpenAI\n\n# Define a simple prompt template as a Python string\n\nprompt_template = PromptTemplate.from_template(\"\"\"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is {concept}?\"\n\n\"\"\")\n\nprompt = prompt_template.format(concept=\"Kmeans clustering\")\n\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:29:33.048906Z","iopub.execute_input":"2024-06-06T07:29:33.049816Z","iopub.status.idle":"2024-06-06T07:30:06.396253Z","shell.execute_reply.started":"2024-06-06T07:29:33.049780Z","shell.execute_reply":"2024-06-06T07:30:06.395235Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.2.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_core-0.2.4-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.74-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m690.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\nDownloading langchain-0.2.2-py3-none-any.whl (973 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.6/973.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.4-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.4/310.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\nDownloading langsmith-0.1.74-py3-none-any.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.2 langchain-core-0.2.4 langchain-text-splitters-0.2.1 langsmith-0.1.74 orjson-3.10.3 packaging-23.2\nCollecting langchain-community\n  Downloading langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: langchain-core in /opt/conda/lib/python3.10/site-packages (0.2.4)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.4)\nRequirement already satisfied: langchain<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.2.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.1.74)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.2.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (2.5.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nDownloading langchain_community-0.2.3-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: langchain-community\nSuccessfully installed langchain-community-0.2.3\n\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Kmeans clustering?\"\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras_nlp\nimport keras_nlp\nsampler1 = keras_nlp.samplers.TopKSampler(k=5,seed = 3, temperature = 0.5)\n#sampler3 = keras_nlp.samplers.RandomSampler()\nsampler2 = keras_nlp.samplers.TopPSampler(p=0.1, k=1_000,seed = 2, temperature = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:35:19.477354Z","iopub.execute_input":"2024-06-06T07:35:19.477782Z","iopub.status.idle":"2024-06-06T07:35:33.813570Z","shell.execute_reply.started":"2024-06-06T07:35:19.477748Z","shell.execute_reply":"2024-06-06T07:35:33.812580Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras_nlp in /opt/conda/lib/python3.10/site-packages (0.12.1)\nRequirement already satisfied: keras-core in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (0.1.7)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (23.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2023.12.25)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (13.7.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (0.1.8)\nRequirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (0.2.3)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2.15.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras_nlp) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras_nlp) (4.66.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core->keras_nlp) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core->keras_nlp) (3.10.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (2.17.2)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (2.15.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.51.1)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp)\n  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text->keras_nlp) (2.15.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.2.2)\nUsing cached keras-2.15.0-py3-none-any.whl (1.7 MB)\nInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#See the object created for sampler1\nsampler1 = keras_nlp.samplers.TopKSampler(k=5,seed = 2, temperature = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:06:04.373940Z","iopub.execute_input":"2024-06-06T08:06:04.374774Z","iopub.status.idle":"2024-06-06T08:06:04.381500Z","shell.execute_reply.started":"2024-06-06T08:06:04.374736Z","shell.execute_reply":"2024-06-06T08:06:04.380560Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"TopKSampler PreTrained","metadata":{}},{"cell_type":"code","source":"gemma_lm.compile(sampler=sampler1)#topK\nprint(gemma_lm.generate(prompt, max_length=128))","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:06:19.094349Z","iopub.execute_input":"2024-06-06T08:06:19.095291Z","iopub.status.idle":"2024-06-06T08:06:43.028725Z","shell.execute_reply.started":"2024-06-06T08:06:19.095256Z","shell.execute_reply":"2024-06-06T08:06:43.027784Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"W0000 00:00:1717661200.676588      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717661200.859665      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Kmeans clustering?\"\n\nSure, here's the explanation you requested:\n\n**What is K-means clustering?**\n\nK-means clustering is a supervised machine learning algorithm that groups data points into a specified number of clusters based on their similarity. The algorithm assumes that data points in\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_lm.compile(sampler = sampler2)\nprint(gemma_lm.generate(prompt, max_length = 512))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:47:39.538783Z","iopub.execute_input":"2024-05-25T19:47:39.539244Z","iopub.status.idle":"2024-05-25T19:48:29.085166Z","shell.execute_reply.started":"2024-05-25T19:47:39.539217Z","shell.execute_reply":"2024-05-25T19:48:29.084172Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"W0000 00:00:1716666490.495117      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Kmeans clustering?\"\n\nSure, here's a detailed explanation of K-means clustering:\n\n**K-means clustering** is a supervised machine learning algorithm used for data partitioning. It is a technique for grouping similar data points into clusters based on their characteristics.\n\n**Key terms:**\n\n* **Cluster:** A group of data points with similar characteristics.\n* **Centroid:** The center point of a cluster, representing the most central data point in the cluster.\n* **Distance:** The distance between two data points, measured by the Euclidean distance in a multi-dimensional space.\n* **Distance metric:** The metric used to measure the distance between data points, such as the Euclidean distance.\n* **K-means algorithm:** A clustering algorithm that iteratively assigns data points to clusters based on their distance to the cluster centers.\n\n**Steps of K-means clustering:**\n\n1. **Choose the number of clusters (k):** Specify the number of clusters you want to create in the data.\n2. **Randomly select k data points as initial cluster centroids.** These points will serve as the centers of the clusters.\n3. **Calculate the distance between each data point and each centroid.** This is done using the distance metric.\n4. **Assign each data point to the nearest centroid.** The data point with the smallest distance to a centroid is assigned to that centroid.\n5. **Re-calculate the centroids by taking the average of the data points in each cluster.**\n6. **Repeat steps 3-5 until the centroids no longer change or until a specified number of iterations is reached.**\n\n**Applications of K-means clustering:**\n\n* **Market segmentation:** Grouping customers based on their purchase history and demographics.\n* **Customer segmentation:** Grouping customers based on their behavior and preferences.\n* **Image segmentation:** Grouping pixels with similar characteristics into regions.\n* **Anomaly detection:** Identifying data points that deviate from the normal pattern.\n\n**Advantages of K-means clustering:**\n\n* **Simple and intuitive algorithm.**\n* **Robust to noise and outliers.**\n* **Can be used with any number of features.**\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color = 'blue'> **Ok, honestly I was expecting a lot worse!**\n\nNeither of those are too bad, but since that's just the pretrained model, I know we can do better. Let's take a look at how to tune Gemma.","metadata":{}},{"cell_type":"markdown","source":"### <font color = 'green'>📈 Tuning the Model ⚙️","metadata":{}},{"cell_type":"markdown","source":"The pretrained version of Gemma isn't trained on a specific task related to data science. Therefore, to address the tasks for this competition, we will want to do some instruction turning. We can train the model with our own data to demonstrate the desired conversational input and output.","metadata":{}},{"cell_type":"markdown","source":"I'm new to tuning LLMs, so I found this great [video](https://www.youtube.com/watch?v=_xxGMSVLwU8) to use as a tutorial for myself for fine-tining Gemma. The video uses Google Colab and I'm not going to use the exact same protocol, but I wanted to use this as a baseline. (I'm going to embed the video below for easy access).","metadata":{}},{"cell_type":"markdown","source":"I'm also going to refer to [this article](https://medium.com/@bavalpreetsinghh/gemma-fine-tuning-using-lora-low-rank-adaptation-dd91fe8bf122) for fine tuning with LoRA.","metadata":{}},{"cell_type":"code","source":"from IPython.display import HTML\nimport warnings\nwarnings.filterwarnings('ignore')\nHTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_xxGMSVLwU8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:29.086841Z","iopub.execute_input":"2024-05-25T19:48:29.087283Z","iopub.status.idle":"2024-05-25T19:48:29.094545Z","shell.execute_reply.started":"2024-05-25T19:48:29.087244Z","shell.execute_reply":"2024-05-25T19:48:29.093612Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_xxGMSVLwU8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"},"metadata":{}}]},{"cell_type":"markdown","source":"I'm pretty sure that this walks through the same code he uses in the video: https://ai.google.dev/gemma/docs/lora_tuning","metadata":{}},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"torch\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:29.095894Z","iopub.execute_input":"2024-05-25T19:48:29.096604Z","iopub.status.idle":"2024-05-25T19:48:29.102008Z","shell.execute_reply.started":"2024-05-25T19:48:29.096569Z","shell.execute_reply":"2024-05-25T19:48:29.101160Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Downloading the file from [Huggingface](https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl)","metadata":{}},{"cell_type":"code","source":"{\n    \"tags\": [\n        \"hide-input\",\n    ]\n}\n\n!wget -O  databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:29.103006Z","iopub.execute_input":"2024-05-25T19:48:29.103330Z","iopub.status.idle":"2024-05-25T19:48:30.810449Z","shell.execute_reply.started":"2024-05-25T19:48:29.103308Z","shell.execute_reply":"2024-05-25T19:48:30.809272Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"--2024-05-25 19:48:30--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\nResolving huggingface.co (huggingface.co)... 18.244.202.118, 18.244.202.60, 18.244.202.68, ...\nConnecting to huggingface.co (huggingface.co)|18.244.202.118|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1716925710&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjkyNTcxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QQIkz5YgSo7lNCkF4PUSBi%7ELr2vgw2R2X1eHVLYb3Sj5Osz7AFfuFOmkFX51BmR5lW8oS9luOZs01GYQt42NbC7pWYS-ylIEJ8-67aSWdkeUZB8UE9OR4MSAPfG9Up7dxKff2n0E3YcHiMw9svlZaflVqLuaMpnG5uzJvbF2eR2YeB3s-cmUebb-TyWs2MXOuiCURKb9v24EUw3kU46YIlSjKLDtNVNKRBBorXigRUfg3pQskH5HW2FxRSr-RZlpXW9rQiQELhmPViMa6Uv%7E2ApFCAFNkuUoeCUanhE4VxyGi26KOrlCe0aqJB4wki6SOPOV-22F%7EP1Lo4yLd7-StA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n--2024-05-25 19:48:30--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8''databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1716925710&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjkyNTcxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QQIkz5YgSo7lNCkF4PUSBi~Lr2vgw2R2X1eHVLYb3Sj5Osz7AFfuFOmkFX51BmR5lW8oS9luOZs01GYQt42NbC7pWYS-ylIEJ8-67aSWdkeUZB8UE9OR4MSAPfG9Up7dxKff2n0E3YcHiMw9svlZaflVqLuaMpnG5uzJvbF2eR2YeB3s-cmUebb-TyWs2MXOuiCURKb9v24EUw3kU46YIlSjKLDtNVNKRBBorXigRUfg3pQskH5HW2FxRSr-RZlpXW9rQiQELhmPViMa6Uv~2ApFCAFNkuUoeCUanhE4VxyGi26KOrlCe0aqJB4wki6SOPOV-22F~P1Lo4yLd7-StA__&Key-Pair-Id=KVTP0A1DKRTAX\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.244.202.38, 18.244.202.35, 18.244.202.14, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.244.202.38|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13085339 (12M) [text/plain]\nSaving to: 'databricks-dolly-15k.jsonl'\n\ndatabricks-dolly-15 100%[===================>]  12.48M  75.6MB/s    in 0.2s    \n\n2024-05-25 19:48:30 (75.6 MB/s) - 'databricks-dolly-15k.jsonl' saved [13085339/13085339]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# read in data\nimport pandas as pd\nraw_data = pd.read_json(\"databricks-dolly-15k.jsonl\",\n                        lines=True,\n                        orient='columns')\nprint(raw_data.shape)\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:30.813087Z","iopub.execute_input":"2024-05-25T19:48:30.814036Z","iopub.status.idle":"2024-05-25T19:48:30.986424Z","shell.execute_reply.started":"2024-05-25T19:48:30.814003Z","shell.execute_reply":"2024-05-25T19:48:30.985508Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"(15011, 4)\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                                                                    instruction  \\\n0                                                    When did Virgin Australia start operating?   \n1                                                      Which is a species of fish? Tope or Rope   \n2                                                Why can camels survive for long without water?   \n3  Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?   \n4                                                               When was Tomoaki Komorida born?   \n\n                                                                                                                                                                                                                                                                                                       context  \\\n0  Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a maj...   \n1                                                                                                                                                                                                                                                                                                                \n2                                                                                                                                                                                                                                                                                                                \n3                                                                                                                                                                                                                                                                                                                \n4  Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002,...   \n\n                                                                                                     response  \\\n0  Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.   \n1                                                                                                        Tope   \n2   Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.   \n3                                                                     The name of the third daughter is Alice   \n4                                                                  Tomoaki Komorida was born on July 10,1981.   \n\n         category  \n0       closed_qa  \n1  classification  \n2         open_qa  \n3         open_qa  \n4       closed_qa  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>context</th>\n      <th>response</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When did Virgin Australia start operating?</td>\n      <td>Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a maj...</td>\n      <td>Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.</td>\n      <td>closed_qa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which is a species of fish? Tope or Rope</td>\n      <td></td>\n      <td>Tope</td>\n      <td>classification</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why can camels survive for long without water?</td>\n      <td></td>\n      <td>Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.</td>\n      <td>open_qa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?</td>\n      <td></td>\n      <td>The name of the third daughter is Alice</td>\n      <td>open_qa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>When was Tomoaki Komorida born?</td>\n      <td>Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002,...</td>\n      <td>Tomoaki Komorida was born on July 10,1981.</td>\n      <td>closed_qa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install jsonlines","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:30.987696Z","iopub.execute_input":"2024-05-25T19:48:30.988004Z","iopub.status.idle":"2024-05-25T19:48:43.622899Z","shell.execute_reply.started":"2024-05-25T19:48:30.987978Z","shell.execute_reply":"2024-05-25T19:48:43.621750Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Collecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_data=raw_data[raw_data['context'].isna()==False] #filter so we only have data with context","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:43.624414Z","iopub.execute_input":"2024-05-25T19:48:43.624738Z","iopub.status.idle":"2024-05-25T19:48:43.645938Z","shell.execute_reply.started":"2024-05-25T19:48:43.624709Z","shell.execute_reply":"2024-05-25T19:48:43.644955Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import pandas\n\n\nfile_to_write = \"\"\nfor index in raw_data.index:\n    raw_data.loc[index].to_json(\"row{}.json\".format(index))\n    with open(\"row{}.json\".format(index)) as file_handle:\n        file_content = file_handle.read()\n        file_to_write += file_content + \"\\n\"\n        \nwith open(\"result.jsonl\",\"w\") as file_handle:\n    file_handle.write(file_to_write)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:43.647205Z","iopub.execute_input":"2024-05-25T19:48:43.647562Z","iopub.status.idle":"2024-05-25T19:48:48.146534Z","shell.execute_reply.started":"2024-05-25T19:48:43.647529Z","shell.execute_reply":"2024-05-25T19:48:48.145700Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import json\ndata = []\nwith open(\"result.jsonl\") as file:\n    for line in file:\n        features = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if features[\"context\"]:\n            continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        data.append(template.format(**features))\n\n# Only use 1000 training examples, to keep it fast.\ndata = data[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:48.147759Z","iopub.execute_input":"2024-05-25T19:48:48.148120Z","iopub.status.idle":"2024-05-25T19:48:48.303303Z","shell.execute_reply.started":"2024-05-25T19:48:48.148088Z","shell.execute_reply":"2024-05-25T19:48:48.302453Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(data)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:32:38.252925Z","iopub.execute_input":"2024-05-25T19:32:38.253315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:48.305877Z","iopub.execute_input":"2024-05-25T19:48:48.306194Z","iopub.status.idle":"2024-05-25T19:48:48.475141Z","shell.execute_reply.started":"2024-05-25T19:48:48.306168Z","shell.execute_reply":"2024-05-25T19:48:48.474183Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Limit the input sequence length to 200 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 200\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=3e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:48:48.476260Z","iopub.execute_input":"2024-05-25T19:48:48.476534Z","iopub.status.idle":"2024-05-25T20:01:42.339513Z","shell.execute_reply.started":"2024-05-25T19:48:48.476510Z","shell.execute_reply":"2024-05-25T20:01:42.338528Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"W0000 00:00:1716666592.787480     131 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 667ms/step - loss: 1.3625 - sparse_categorical_accuracy: 0.4859\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7acb451f0bb0>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now let's try to prompt the turned model with the same prompts that we used earlier. The only difference is I'm going to increase the max length to 1000.","metadata":{}},{"cell_type":"markdown","source":"**TopKSampler after Tuning**","metadata":{}},{"cell_type":"code","source":"gemma_lm.compile(sampler=sampler1)\nprint(gemma_lm.generate(prompt, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:01:42.342782Z","iopub.execute_input":"2024-05-25T20:01:42.343104Z","iopub.status.idle":"2024-05-25T20:02:48.644444Z","shell.execute_reply.started":"2024-05-25T20:01:42.343078Z","shell.execute_reply":"2024-05-25T20:02:48.643412Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"W0000 00:00:1716667334.920816      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Kmeans clustering?\"\n\nAnswer:\n\nSure, here's a detailed explanation of the K-means clustering concept:\n\n**What is K-means clustering?**\n\nK-means clustering is a technique for partitioning a set of data points into a specified number of clusters (k). The data points are then assigned to a cluster based on their similarity to the cluster's center.\n\n**Key terms:**\n\n* **Centroid:** The center of a cluster. The centroid is the point that is furthest from all other points in the cluster.\n* **Distance:** The distance between a data point and a centroid.\n* **Distance metric:** The distance metric is a measure of similarity between two data points. The most common distance metric is the Euclidean distance, which is the straight-line distance between two points.\n* **K-means algorithm:** The k-means algorithm works by iteratively assigning data points to clusters. At each iteration, the data point with the smallest distance to the centroid of its cluster is assigned to that cluster. The process continues until each cluster contains the same number of data points.\n* **Cluster centroids:** The centroids are the centers of the clusters. The cluster centroids are the points that are furthest from all other points in the cluster.\n\n**How to perform k-means clustering:**\n\n1. **Choose the number of clusters k.** This is typically done by looking at the data and knowing how many clusters you expect.\n2. **Choose a distance metric.** The most common distance metric is the Euclidean distance, which is the straight-line distance between two points.\n3. **Select the centroids.** The centroids are the centers of the clusters. You can choose the centroids randomly, or you can use a different method for selecting the centroids.\n4. **Iterate over the data points.** For each data point, assign it to a cluster based on the distance from the centroid of its cluster.\n5. **Repeat steps 2 and 3 until each data point is assigned to a cluster.**\n6. **Calculate the cluster centroids.** The cluster centroids are the points that are furthest from all other points in the cluster.\n7. **Repeat steps 2 and 3 to create new clusters.** Continue to do this until the centroids are no longer changing.\n\n**Applications of K-means clustering:**\n\nK-means clustering is used in a variety of applications, including:\n\n* **Market segmentation:** K-means clustering can be used to segment a market into different groups of customers. This can be used to target marketing campaigns and to develop new products and services.\n* **Customer segmentation:** K-means clustering can be used to segment a customer base into different groups of customers. This can be used to develop new product and service offerings, to develop new marketing campaigns, and to improve customer service.\n* **Image segmentation:** K-means clustering can be used to segment an image into different regions. This can be used for a variety of purposes, such as object recognition, medical image analysis, and security.\n* **Anomaly detection:** K-means clustering can be used to identify outliers in a dataset. Outliers are data points that are far away from the rest of the data points. This can be used for a variety of purposes, such as fraud detection, quality control, and security.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**TopPSampler after Tuning**","metadata":{}},{"cell_type":"code","source":"gemma_lm.compile(sampler=sampler2)\nprint(gemma_lm.generate(prompt, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:02:48.645506Z","iopub.execute_input":"2024-05-25T20:02:48.645780Z","iopub.status.idle":"2024-05-25T20:03:34.207647Z","shell.execute_reply.started":"2024-05-25T20:02:48.645756Z","shell.execute_reply":"2024-05-25T20:03:34.206685Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"W0000 00:00:1716667394.426386      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Kmeans clustering?\"\n\nAnswer: K-means clustering is a technique for grouping data points into a specified number of clusters. The clusters are formed based on the similarity of the data points. The data points are then assigned to the cluster with the closest center. The center is the point that is most similar to all of the data points in the cluster.\n\nThe number of clusters is chosen by the user. The user can also specify the distance metric to be used for the clustering. The most common distance metric is the Euclidean distance, which is the distance between two points in space.\n\nThe K-means clustering algorithm works by iterating over the data points and assigning each data point to a cluster. The cluster with the closest center is assigned to the cluster. The algorithm continues until each data point is assigned to a cluster or until the number of clusters is reached.\n\nThe K-means clustering algorithm is a powerful technique for grouping data points. It can be used to identify patterns in data and to make predictions about data. The algorithm is also very efficient, and it can be used to cluster large datasets in a matter of hours.\n\nHere are some of the key terms associated with K-means clustering:\n\n* **Cluster:** A group of data points that are similar to each other.\n* **Center:** The point that is most similar to all of the data points in a cluster.\n* **Distance metric:** A measure of similarity between two data points. The most common distance metric is the Euclidean distance, which is the distance between two points in space.\n* **K-means clustering algorithm:** A technique for grouping data points into a specified number of clusters.\n* **Cluster centroids:** The center of each cluster.\n* **Distance to cluster center:** The distance from a data point to the center of its cluster.\n* **Distance to cluster centroid:** The distance from a data point to the center of its cluster.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Initial Findings 📌","metadata":{}},{"cell_type":"markdown","source":"By using the context-based data to train and fine tune Gemma, I recieved different results from my prompts. However, I'm not sure if the fine tuned model is better or worse than the original. I will note that the tuned model is formatted in a clearer way due to the data that we fed it. (it doens't have all the asterisks)","metadata":{}},{"cell_type":"markdown","source":"Perhaps we should test out a few other prompts?","metadata":{}},{"cell_type":"markdown","source":"</b>I like the topKSampler after tuning, so let's try another prompt with that one. All we need to do is enter a new \"concept\".<b>","metadata":{}},{"cell_type":"code","source":"prompt = prompt_template.format(concept=\"Supervised learning\")\n\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:08:25.550432Z","iopub.execute_input":"2024-05-25T20:08:25.550812Z","iopub.status.idle":"2024-05-25T20:08:25.556380Z","shell.execute_reply.started":"2024-05-25T20:08:25.550783Z","shell.execute_reply":"2024-05-25T20:08:25.555298Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Supervised learning?\"\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_lm.compile(sampler=sampler1)\nprint(gemma_lm.generate(prompt, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:08:49.253294Z","iopub.execute_input":"2024-05-25T20:08:49.253674Z","iopub.status.idle":"2024-05-25T20:09:30.069146Z","shell.execute_reply.started":"2024-05-25T20:08:49.253645Z","shell.execute_reply":"2024-05-25T20:09:30.068071Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"W0000 00:00:1716667751.264379      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Supervised learning?\"\n\nAnswer: Supervised learning is a machine learning technique that involves training a model on labeled data. This means that each data point has a label, which is a known outcome or target variable. The model learns to make accurate predictions on new, unseen data.\n\nSupervised learning can be divided into two main types: supervised machine learning and unsupervised machine learning. In supervised learning, the data is labeled by humans, who provide the correct answers to the model. In unsupervised learning, the model is not labeled by humans. Instead, it learns to find patterns and relationships in the data on its own.\n\nSupervised learning algorithms include linear regression, logistic regression, and decision trees. These algorithms are used to find the best fit line or decision boundary between two or more variables.\n\nIn supervised learning, the model is trained on a set of labeled data. The model uses these labeled data to learn the relationship between the input and output variables. Once the model is trained, it can be used to make predictions on new, unseen data.\n\nHere are some of the key terms associated with supervised learning:\n\n* **Labeled data:** Data that has been manually tagged by humans. This data is used to train the model.\n* **Model:** A mathematical representation of the relationships between the input and output variables.\n* **Training data:** The data that is used to train the model.\n* **Test data:** The data that is used to evaluate the model's performance.\n* **Feature:** A variable that is used to make a prediction.\n* **Target variable:** The variable that is used to make a prediction.\n\nSupervised learning is a powerful technique that can be used to build accurate models that can be used to make predictions. However, supervised learning can be computationally expensive, and it can be difficult to find labeled data that is high quality.\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_lm.compile(sampler=sampler2)\nprint(gemma_lm.generate(prompt, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:09:30.071078Z","iopub.execute_input":"2024-05-25T20:09:30.071529Z","iopub.status.idle":"2024-05-25T20:10:03.412346Z","shell.execute_reply.started":"2024-05-25T20:09:30.071494Z","shell.execute_reply":"2024-05-25T20:10:03.411243Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"W0000 00:00:1716667794.389575      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Supervised learning?\"\n\nAnswer: Supervised learning is a machine learning technique that uses labeled data to teach a model to make predictions on new, unseen data. The data is divided into two sets: training and testing. The training set is used to build the model, while the testing set is used to evaluate the model's performance.\n\nSupervised learning algorithms include linear regression, logistic regression, and decision trees. Each algorithm has its own strengths and weaknesses, but they all work by finding a linear relationship between the input and output variables. The model is then trained to minimize the error between the predicted and actual values.\n\nSupervised learning is a powerful technique that can be used to build accurate and reliable models. However, it is important to note that supervised learning algorithms require labeled data, which can be expensive and time-consuming to collect.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"gemma_lm.compile(sampler=sampler3)\nprint(gemma_lm.generate(prompt, max_length=500))### Evaluating the Quality of Response ✔️ ❌","metadata":{}},{"cell_type":"markdown","source":"I'm not as satisfied with this answer. I wonder if we can adjust the temperature to keep the answers a little more consistent? However, I'm also going to increase k to 7 to strike a balance between the reliability of answers and the variety of tokens feeding forward.","metadata":{}},{"cell_type":"code","source":"sampler3 = keras_nlp.samplers.TopKSampler(k=7, temperature=.4, seed=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:10:03.596365Z","iopub.execute_input":"2024-05-25T20:10:03.596739Z","iopub.status.idle":"2024-05-25T20:10:03.603995Z","shell.execute_reply.started":"2024-05-25T20:10:03.596701Z","shell.execute_reply":"2024-05-25T20:10:03.603145Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"gemma_lm.compile(sampler=sampler3)\nprint(gemma_lm.generate(prompt, max_length=500))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:10:08.948706Z","iopub.execute_input":"2024-05-25T20:10:08.949059Z","iopub.status.idle":"2024-05-25T20:10:51.951115Z","shell.execute_reply.started":"2024-05-25T20:10:08.949032Z","shell.execute_reply":"2024-05-25T20:10:51.950059Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"W0000 00:00:1716667840.285081      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1716667840.819808      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is Supervised learning?\"\n\nAnswer: Supervised learning is a type of machine learning that involves the training of a model on a dataset of labeled data. The model learns to make predictions on new data that is similar to the training data. This type of learning is used for tasks such as classification, regression, and clustering.\n\nThe training data consists of a set of labeled data points, where each data point is associated with a known outcome. The model learns to map the input data to the output labels. This process is called supervised learning because the model is given a set of labeled data to work with.\n\nSupervised learning can be divided into two main types: supervised and unsupervised learning. In supervised learning, the model is given a set of labeled data points and learns to make predictions on new data. In unsupervised learning, the model is given a set of unlabeled data points and learns to find patterns and relationships in the data.\n\nSupervised learning is a powerful technique that can be used to build accurate and robust models. However, supervised learning can be computationally expensive, especially for large datasets. Additionally, the quality of the training data can have a significant impact on the performance of the model.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's focus on a use case for a Gemma Data Science Tutoring assistant...","metadata":{}},{"cell_type":"markdown","source":"# How Can the Gemma Data Science Tutor Help?","metadata":{}},{"cell_type":"markdown","source":"As a new Kaggler, you might stumble on some questions as you're working through your Spaceship Titanic notebook. Let's consider a few below:","metadata":{}},{"cell_type":"markdown","source":"You're new to the Kaggle platform and you're trying out the Getting Started competitions. Let's say that you're trying to participate in the Spaceship Titanic competition. [Here's an overview of the contest](https://www.kaggle.com/competitions/spaceship-titanic):\n\nThis is a spin on the Titanic Classification contest. Instead of predicting which passengers survive on the Titanic, you are predicting \"which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\"\nThe evaluation metric is classification accuracy.\nThe training data includes records for about 8,700 passengers. There are 14 columns in the training set. The target variable is the Transported value (true or false).","metadata":{}},{"cell_type":"markdown","source":"![Sky Image](https://www.kaggle.com/competitions/34377/images/header)","metadata":{}},{"cell_type":"markdown","source":"### 1. What is classification accuracy?","metadata":{}},{"cell_type":"code","source":"prompt = prompt_template.format(concept=\"classification accuracy\")\n\nprint(prompt)\ngemma_lm.compile(sampler=sampler1)\nprint(gemma_lm.generate(prompt, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:18:20.170636Z","iopub.execute_input":"2024-05-25T20:18:20.171006Z","iopub.status.idle":"2024-05-25T20:19:04.658103Z","shell.execute_reply.started":"2024-05-25T20:18:20.170978Z","shell.execute_reply":"2024-05-25T20:19:04.657153Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is classification accuracy?\"\n\n\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716668323.975690      35 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. I will ask you a question about a data science topic and I want you to generate up to 500 words to explain the concept.The explanation should use definitions of key terms. Question: What is classification accuracy?\"\n\nAnswer: Classification accuracy is a measure of how well a data science model can correctly classify the data it is trained on. It is measured by dividing the number of correctly classified data points by the total number of data points in the dataset and multiplying by 100.\n\nThe following are some of the key terms that are used when discussing classification accuracy:\n\n* Precision: Precision is the rate at which the model correctly identifies positive instances. It is calculated by dividing the number of correctly classified positive instances by the total number of positive instances in the dataset.\n* Recall: Recall is the rate at which the model correctly identifies negative instances. It is calculated by dividing the number of correctly classified negative instances by the total number of negative instances in the dataset.\n* False positives: False positives are instances that the model incorrectly identifies as positive. False positives can be caused by the model misclassifying instances that are actually negative or by the model misclassifying instances that are actually positive.\n* False negatives: False negatives are instances that the model incorrectly identifies as negative. False negatives can be caused by the model misclassifying instances that are actually positive or by the model misclassifying instances that are actually negative.\n\nThe classification accuracy of a data science model is a measure of how well the model can learn the underlying structure of the data. A model with high classification accuracy is able to correctly classify new data points that are similar to the training data. A model with low classification accuracy is able to misclassify new data points that are similar to the training data.\n\nThere are a number of different factors that can affect the classification accuracy of a data science model, including the size of the dataset, the complexity of the data, and the type of data. In general, the larger the dataset, the more complex the data, and the more difficult it is to classify the data, the lower the classification accuracy.\n\nThe classification accuracy of a data science model can be used to evaluate the performance of the model and to identify areas where the model can be improved.\n","output_type":"stream"}]},{"cell_type":"code","source":"follow_up_prompt = PromptTemplate.from_template(\"\"\"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. you just explained a data science conecpt to me and I have a follow up question about how to apply the concept. Please answer the following question and provide python code snippets as needed. {Question}?\"\n\n\"\"\")\n\nprompt2 = follow_up_prompt.format(Question=\"How can I calculate the accuracy of my classification model in python?\")\n\nprint(prompt2)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:19:04.660005Z","iopub.execute_input":"2024-05-25T20:19:04.660326Z","iopub.status.idle":"2024-05-25T20:19:04.666198Z","shell.execute_reply.started":"2024-05-25T20:19:04.660301Z","shell.execute_reply":"2024-05-25T20:19:04.665205Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. you just explained a data science conecpt to me and I have a follow up question about how to apply the concept. Please answer the following question and provide python code snippets as needed. How can I calculate the accuracy of my classification model in python??\"\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(gemma_lm.generate(prompt2, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:19:04.667408Z","iopub.execute_input":"2024-05-25T20:19:04.667728Z","iopub.status.idle":"2024-05-25T20:19:23.955258Z","shell.execute_reply.started":"2024-05-25T20:19:04.667701Z","shell.execute_reply":"2024-05-25T20:19:23.954168Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. you just explained a data science conecpt to me and I have a follow up question about how to apply the concept. Please answer the following question and provide python code snippets as needed. How can I calculate the accuracy of my classification model in python??\"\n\nAnswer: Sure. Here is a follow up question about the concept you just explained and how to apply it in Python:\n\n**Question:**\nHow can I calculate the accuracy of my classification model in python?\n\n**Answer:**\nSure, here's how you can calculate the accuracy of your classification model in Python:\n\n```python\nfrom sklearn.metrics import accuracy_score\n\n# True positives, False positives, True negatives, False negatives\ny_true = [1, 0, 1, 0]  # True labels\ny_pred = [1, 0, 1, 0]  # Predicted labels\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n```\n\n**Explanation:**\n1. Import the `accuracy_score` function from the `sklearn.metrics` module.\n2. Define the true labels `y_true` and the predicted labels `y_pred`.\n3. Call the `accuracy_score` function with the `y_true` and `y_pred` arguments.\n4. Print the accuracy of the model.\n\n**Note:**\nThe accuracy of a classification model is a measure of how well it can correctly classify new data points. It is calculated by comparing the predicted labels to the true labels and then dividing by the total number of data points in the dataset.\n\n**Additional Tips:**\n* You can also calculate the confusion matrix, precision, and recall to get a more comprehensive understanding of the model's performance.\n* You can use the `cross_val_score` function to get a more robust estimate of the model's accuracy.\n* You can also use the `auc` (area under the curve) metric to compare different classification models.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. How can I deal with missing values for a classification model?","metadata":{}},{"cell_type":"markdown","source":"If you look at the Spaceship Titantic data, you'll see that there are instances of missing data. Specifically, the Cabin column has 2% NULL values. As a new Kaggler, you might want to know how to handle these values. Let's see what Gemma suggests...","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*xU1nY7sCPSoPsyLh\" alt=\"Image Description\" style=\"display: block; margin-left: auto; margin-right: auto;\">","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:21:11.846462Z","iopub.execute_input":"2024-05-25T20:21:11.847188Z","iopub.status.idle":"2024-05-25T20:21:11.853114Z","shell.execute_reply.started":"2024-05-25T20:21:11.847155Z","shell.execute_reply":"2024-05-25T20:21:11.851932Z"}}},{"cell_type":"code","source":"prompt3 = follow_up_prompt.format(Question=\"How can I handle missing values in a classification model?\")\n\nprint(prompt3)\nprint(gemma_lm.generate(prompt3, max_length=1000))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:21:38.016804Z","iopub.execute_input":"2024-05-25T20:21:38.017527Z","iopub.status.idle":"2024-05-25T20:22:03.164735Z","shell.execute_reply.started":"2024-05-25T20:21:38.017495Z","shell.execute_reply":"2024-05-25T20:22:03.163676Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. you just explained a data science conecpt to me and I have a follow up question about how to apply the concept. Please answer the following question and provide python code snippets as needed. How can I handle missing values in a classification model??\"\n\n\n\nContext: You are a data science tutor for an online bootcamp course. You are teaching a student about different data science concepts. you just explained a data science conecpt to me and I have a follow up question about how to apply the concept. Please answer the following question and provide python code snippets as needed. How can I handle missing values in a classification model??\"\n\nSure, here's the answer to your question:\n\n**Handling Missing Values in a Classification Model**\n\nMissing values in a dataset can be a challenge for classification models, as they can affect the model's ability to learn and make predictions. There are several approaches to handling missing values, including:\n\n* **Imputation:** This involves filling in missing values with estimated or imputed values based on the available features. Common imputation techniques include mean/median imputation, k-nearest neighbors (KNN) imputation, and multiple imputation.\n* **Deletion:** In some cases, it may be acceptable to delete observations with missing values. This approach is often used when the dataset is relatively small or when there is a high proportion of missing values.\n* **Indicator coding:** This approach is used when there are many missing values that are the same value (e.g., all missing values are missing or all missing values are the same category). Indicator variables are created for each missing value, indicating the category it belongs to.\n* **Modeling:** Some classification models, such as k-nearest neighbors (KNN) and decision trees, can handle missing values by using a technique called k-nearest neighbors (KNN) or decision tree induction.\n\n**Python Code Snippets for Handling Missing Values**\n\n```python\n# Load the missing data\ndata = pd.read_csv(\"missing_data.csv\")\n\n# Impute missing values with mean/median\ndata[\"target_variable\"] = data[\"target_variable\"].fillna(data[\"target_variable\"].mean())\ndata[\"target_variable\"] = data[\"target_variable\"].fillna(data[\"target_variable\"].median())\n\n# Drop rows with missing values\ndata.dropna(inplace=True, subset=[\"target_variable\"])\n\n# Encode missing values as indicator variables\ndata[\"target_variable_indicator\"] = data[\"target_variable\"].astype(int)\n```\n\n**Additional Tips for Handling Missing Values**\n\n* Before imputing missing values, it's important to understand the underlying mechanism of the missing data. This can help you choose an appropriate imputation method.\n* It's often useful to have a preliminary understanding of the data and the features that are most likely to be missing. This can help you decide on an appropriate imputation method.\n* It's important to evaluate the impact of missing values on the model's performance. This can help you determine the best approach for handling missing values.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## How did Gemma do?","metadata":{}},{"cell_type":"markdown","source":"I don't think this is a bad response! Gemma goes over how to find the number of missing values in a column as well as how to impute missing vvalues iwth the mean, median, or mode. It also covers the common practice of filling NaN values with 0. In addition, Gemma provides an example of imputing missing values with KNN.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i0.wp.com/thesewaneepurple.org/wp-content/uploads/2018/11/a-plus-school-letter-grade.jpg?resize=840%2C560&ssl=1\" alt=\"Image Description\" style=\"display: block; margin-left: auto; margin-right: auto;\">","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:22:44.837166Z","iopub.execute_input":"2024-05-25T20:22:44.837784Z","iopub.status.idle":"2024-05-25T20:22:44.843681Z","shell.execute_reply.started":"2024-05-25T20:22:44.837753Z","shell.execute_reply":"2024-05-25T20:22:44.842415Z"}}},{"cell_type":"markdown","source":"<img src=\"https://helios-i.mashable.com/imagery/articles/00veJ5qeI90cfXdfFzUfCrv/hero-image.fill.size_1248x702.v1708464912.jpg\" alt=\"Image Description\" style=\"display: block; margin-left: auto; margin-right: auto;\">","metadata":{"execution":{"iopub.status.busy":"2024-05-25T20:24:39.666026Z","iopub.execute_input":"2024-05-25T20:24:39.666655Z","iopub.status.idle":"2024-05-25T20:24:39.671778Z","shell.execute_reply.started":"2024-05-25T20:24:39.666629Z","shell.execute_reply":"2024-05-25T20:24:39.670472Z"}}},{"cell_type":"markdown","source":"<img src=\"https://media.istockphoto.com/id/1300123069/photo/runners-running-towards-the-finish-line.jpg?s=612x612&w=0&k=20&c=XNlsYwQWlHHIlNYaxLuJU-YrD46ZTdPh9WWQpOt6Z60=\" alt=\"Image Description\" style=\"display: block; margin-left: auto; margin-right: auto;\">","metadata":{}}]}